"""
AUTHENTIC PRISM Scoring Service - 6 Dimensions
Implements the PRISM methodology with 6 dimensions as specified
Uses LLM-based scoring with Impact as a single dimension
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import json
import re
from datetime import datetime
import openai
from openai import OpenAI
import os
from tenacity import retry, stop_after_attempt, wait_random_exponential
import json_repair
from app.core.config import settings

class PRISMScorer:
    """
    Authentic PRISM scoring engine using 6 dimensions
    Based on the agentic-score.ipynb implementation but with Impact as single dimension
    """
    
    def __init__(self):
        # Initialize OpenAI client - prioritize .env file over system environment
        # Ensure we use the values from our .env file
        api_key = settings.OPENAI_API_KEY
        base_url = settings.OPENAI_BASE_URL
        
        # Fallback to environment only if settings are None
        if not api_key:
            api_key = os.getenv('OPENAI_API_KEY')
        if not base_url:
            base_url = os.getenv('OPENAI_BASE_URL', 'https://api.openai.com/v1')
            
        print(f"PRISM Scorer Initialization:")
        print(f"  API key: {api_key[:15] if api_key else 'None'}...")
        print(f"  Base URL: {base_url}")
        print(f"  Source: {'settings' if settings.OPENAI_API_KEY else 'env'}")
        
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )
        
        # Load authentic prompts from research
        self.few_shot_prompt = self._load_few_shot_examples()
        self.scoring_logic_prompt = self._load_scoring_logic()
        self.router_prompt = self._load_router_prompt()
        self.phd_student_prompt = self._load_phd_student_prompt()
        self.reviewer_persona = self._load_reviewer_personas()
        
    def _load_few_shot_examples(self) -> str:
        """EXACT few-shot examples from research adapted for 6 dimensions"""
        return """
    Example 1: 

    Original Incident System Name (IN1):
    Adobe Firefly

    Original Incident System Description (IS1):
    "Firefly is a generative machine learning model developed and operated by Adobe. Firefly was built on Adobe's Sensei AI platform and was trained with images and videos in Adobe's royalty-free Stock platform and public domain images from Creative Commons, Wikimedia, Flickr Commons, and other generative AI systems. Firefly was launched as a beta in March 2023 and fully released in June 2023. Adobe is in the process of integrating Firefly with a number of its products, including Illustrator, Photoshop, Premiere Pro and Express."

    New Product Name (NM1):
    Adobe Firefly 2

    New Product Description (PD1):
    "Adobe's three generative AI models (Firefly Image 2, Firefly Vector, Firefly Design) add features to Illustrator, Adobe Express, and Photoshop. Use Generative Match (beta) in Firefly Image 2 to generate images based on the look and feel of an existing image."
    
    Incident Description (ID1):
    "Adobe trained its image generating tool, Firefly, on AI images generated by competitor generative AI tools, despite marketing Firefly as a commercially safe and ethically sound image generator."

    Logical Coherence: 5
    Factual Accuracy: 5
    Practical Implementability: 5
    Contextual Relevance: 5
    Impact: 1
    Exploitability: 1
    Application Domain: Art

    Labeling rationale: 
    Logical Coherence: The reported incident of Adobe Firefly is applicable and makes sense to occur to the Firefly 2.
    Factual Accuracy: Firefly 2 uses the same underlying model as Adobe Firefly to offer image-generation capabilities.
    Practical Implementability: This incident could be found in multiple real-world cases (training images generated by other)
    Contextual Relevance: The Adobe Firefly and Firefly 2 are in the exact same application context.
    Impact: The incident has no direct impact on users, groups, or society - minimal harm overall.
    Exploitability: No extra effort is needed to create such risks.

    Example 2: 

    Original Incident System Name (IN1):
    Character AI

    Original Incident System Description (IS1):
    Character AI is a chatbot service that enables users to create "characters" using artificial intelligence, craft their "personalities" and share them for others to interact with. The bots can be based on fictional characters or real people - alive or dead, and has marketed them as "AIs that feel alive" and that "hear you, understand you, and remember you." The service is estimated to have tens of millions of users, most of them teenagers. In 2024, Google signed a non-exclusive agreement to use Character AI's technology.

    New Product Name (NM1):
    Dippy Widget - AI Character Chat

    New Product Description (PD1):
    Dippy brings true AI friends and characters to your Home Screen. Your AI friends know your likes, dislikes, interests, inner thoughts, and encourage you to be the best version of yourself.

    Incident Description (ID1):
    Boy commits suicide after relationship with Character AI chatbot, Anthropomorphism. Anthropomorphism is the attribution of human traits, emotions, or intentions to non-human entities. It is considered to be an innate tendency of human psychology.

    Logical Coherence: 5
    Factual Accuracy: 5
    Practical Implementability: 4
    Contextual Relevance: 4
    Impact: 5
    Exploitability: 3
    Application Domain: AI Companion

    Labeling rationale: 
    Logical Coherence: The incident reported in Character AI could occur with the new product as well, since it supports AI friends and chat-based characters
    Factual Accuracy: The new product leverages the same technology as Character.ai, using a chatbot service that allows users to create custom characters.
    Practical Implementability: The incident itself and the further action that user took (suicide) is rather a person decision.
    Contextual Relevance: While they both are AI companions, character AI can create fictional character, that Dippy can not
    Impact: Suicide is an irreversible harm to individual, which represents the highest impact severity possible.
    Exploitability: Such an outcome (suicide) is a personal decision dependent on user behavior.
        """
    
    def _load_scoring_logic(self) -> str:
        """EXACT scoring logic from research adapted for 6 dimensions"""
        return """

    1. Logical Coherence: 
        Definition: Does the reported incident logically fit the new product's/system's function/purpose directly (Think from the perspective of the overall objective of the new product and the original incident system involved)?
        Compare IS1, ID1 and PD1
        Scoring: 5 - Designed with the same intent, and the incident makes exact sense under that designed purpose 
                4 - The product nature requires minor changes to replicate the incident, but the product was designed for exact same purpose; or the product purpose was modified a bit, but the incident could be exactly the same
                3 - 1) The product nature requires minor changes to replicate the incident, and the product purpose was modified a bit OR 2) The product nature requires major changes to replicate the incident, and the product purpose was the same
                2 - The product nature requires major changes to replicate the incident, and the product purpose was modified a lot
                1 - Incident does not fit in the nature of the product

    2. Factual Accuracy:
        Definition: Is the proposed Incident within the scope of the new products features (Can the incident be replicated based on one of the features or based on the technology used. Think in terms of the underlying technology)?
        Compare ID1 and PD1
        Scoring: 5 -  incident makes exact sense under that technology
                 3 -  incident makes somewhat sense under that technology
                 1 -  incident makes no sense under that technology; the new product is not using the features/ technology that the originally product is using 
    
    3. Practical Implementability:
        Definition: How likely is the incident to occur in real-world conditions?
        Think interms of ID1 and the underlying issue
        Scoring: 5 - (Incident) High Likelihood of occurence
                 4 - (Incident) Relatively lower likelihood of occurence
                 3 - (Incident) Lower Likelihood of occurence 
                 2 - (Incident) unlikely to occur
                 1 - (Incident) Nearly impossible to occur

    4. Contextual Relevance:
        Compare ID1 and PD1
        Definition: Does mapping this incident to the new product make sense in terms of the application domain of the product (Think interms of the application domain)?
        Scoring: 5 - Exact context applies directly to the same use case
                 4 - Similar context (application domain) but minor modifications
                 3 - Similar context but requires major modifications
                 2 - Different contexts (application domain) requiring modifications
                 1 - Contextually inappropriate
    
    5. Impact:
        Definition: How severe is the overall impact across individual, group/community, and global scales?
        Only consider ID1
        Scoring: 5 - Life-threatening, irreversible harm (safety, health, etc) at any scale
                 4 - Significant harm (finance, loss, etc) at any scale
                 3 - Manageable harm 
                 2 - Low impact harm (The product is unusable, but the incident does not impact significantly)
                 1 - Negligible or no impact at any scale
    
    6. Exploitability:
        Definition: Is the risk inherent to the system or does it occur from user misuse?
        Only consider ID1
        Scoring: 5 - Easily manipulated by users with bad intention
                 4 - Unintended mistakes by users
                 3 - Partially dependent on user behavior
                 2 - Hard to be triggered by users alone 
                 1 - System-level, requires no external manipulation
    
 """
    
    def _load_router_prompt(self) -> str:
        """EXACT router prompt from research"""
        return """
Your task is to identify the primary risk type associated with an incident.
Inputs provided are:
- IS1 (Original Incident System Description)
- ID1 (Incident Description)
- PD1 (New Product Description)

Based on these inputs, classify the dominant risk type and provide a brief justification.

You must choose **one** of the following 10 categories that best represents the core risk involved:

- Bias, Discrimination & Fairness  
- Accuracy, Reliability & Robustness  
- Privacy, Confidentiality & Surveillance  
- Safety & Security  
- Transparency, Explainability & Oversight  
- Misinformation & Information Integrity  
- Copyright, IP & Originality  
- Ethics, Values & Appropriateness  
- Freedom of Expression & Human Rights  
- Governance, Compliance & Legal  

```json
{{
  "risk_type": "<string>",
  "justification": "<string>"
}}
```
"""
    
    def _load_phd_student_prompt(self) -> str:
        """EXACT PhD student prompt from research adapted for 6 dimensions"""
        return f"""
You are an expert PhD student working in the field of Ethics and AI risks.
<tasks>
Your task is to simulate a human-like annotation process by assigning ratings and providing brief rationales for each of the following parameters:
- Logical Coherence
- Factual Accuracy
- Practical Implementability
- Contextual Relevance
- Impact
- Exploitability

Additionally, assign an Application Domain.
Each parameter should be returned as a list: [score, rationale], where score is an integer (1-5) based on the following rubric.

Inputs:
- IS1: Original Incident System Description
- ID1: Incident Description
- PD1: New Product Description
</task>

<example>

The following is the rationale used for scoring each parameter:

{self.scoring_logic_prompt}

Below are some few-shot examples to guide your scoring decisions:

{self.few_shot_prompt}


</example>

<inputs>
- IS1 (Original Incident System Description): 
 {{IS1}}

- ID1 (Incident Description):
  {{ID1}}

- PD1 (New Product Description):
  {{PD1}}
</inputs>

While assigning scores, do ensure that every score assigned is completely based on the information provided in the inputs. Do not assume any additional information or context beyond what is provided in the inputs.

Return the result as a JSON object in the following format:
```json
{{
  "Logical Coherence": [<int>, "<rationale>"],
  "Factual Accuracy": [<int>, "<rationale>"],
  "Practical Implementability": [<int>, "<rationale>"],
  "Contextual Relevance": [<int>, "<rationale>"],
  "Impact": [<int>, "<rationale>"],
  "Exploitability": [<int>, "<rationale>"],
  "Application Domain": "<string>"
}}
```
"""
    
    def _load_reviewer_personas(self) -> Dict[str, str]:
        """EXACT reviewer personas from research"""
        return {
            "Bias, Discrimination & Fairness": (
                "You are an AI ethics expert with deep knowledge of fairness auditing, demographic impact assessment, and social justice frameworks. "
                "You specialize in identifying systemic and emergent biases within machine learning systems, especially those affecting race, gender, income, and disability groups. "
                "You are well-versed in legal standards (like the Civil Rights Act) and fairness-aware modeling. "
                "Your reviews prioritize equitable treatment, inclusion, accessibility, and the potential of algorithmic harm to marginalized or vulnerable populations. "
                "You draw from sociology, critical theory, and DEI best practices to assess if the rationale aligns with the observed risks of exclusion, harm, or discrimination."
            ),
            "Accuracy, Reliability & Robustness": (
                "You are a technical AI researcher with deep expertise in testing, benchmarking, and debugging machine learning systems. "
                "You understand model drift, hallucination, overfitting, and generalization issues across diverse deployment settings. "
                "You specialize in assessing whether outputs align with the ground truth, expected behavior, or training-time assumptions. "
                "You evaluate system performance under stress, adversarial inputs, or incomplete data. "
                "Your review ensures that rationales accurately reflect the level of factual accuracy, stability, and real-world reliability of the incident as applied to the new product's architecture or data pipeline."
            ),
            "Privacy, Confidentiality & Surveillance": (
                "You are a privacy specialist with experience in data protection, confidentiality controls, and surveillance ethics. "
                "You are highly familiar with legal frameworks such as GDPR, HIPAA, and CCPA, and how they relate to the collection, use, and sharing of personal data. "
                "You assess whether the rationales correctly interpret risks related to unauthorized data access, identity leakage, consent violations, or long-term surveillance implications. "
                "You are particularly attentive to harms caused by re-identification, metadata misuse, or cross-context inference. "
                "You prioritize ethical data governance, transparency of user rights, and systemic safeguards against both intentional and inadvertent breaches of privacy."
            ),
            "Safety & Security": (
                "You are a cybersecurity and AI safety engineer with expertise in system vulnerabilities, threat modeling, and real-world hazard analysis. "
                "You evaluate whether the rationale correctly identifies potential risks that could cause harm to individuals, communities, or infrastructure. "
                "You specialize in attack surfaces such as data poisoning, prompt injection, and unsafe decision-making under uncertainty. "
                "You assess how likely it is that similar threats can manifest in the new system, and whether safeguards exist. "
                "You are also familiar with high-stakes domains such as autonomous vehicles, healthcare, and defense, where failures can be catastrophic."
            ),
            "Transparency, Explainability & Oversight": (
                "You are a governance and transparency expert with experience in algorithmic accountability, audit trails, explainability, and model interpretability. "
                "You examine whether the rationale meaningfully reflects the availabilityâ€”or lackâ€”of documentation, decision pathways, and oversight mechanisms. "
                "You are focused on whether end users, auditors, or regulators can understand, question, or intervene in a system's output. "
                "You also assess whether black-box behavior, hidden logic, or a lack of oversight introduces governance risk. "
                "You prioritize traceability, procedural transparency, and mechanisms for redress in AI systems operating across sectors."
            ),
            "Misinformation & Information Integrity": (
                "You are a media and NLP researcher focused on the detection and prevention of misinformation, disinformation, and manipulated content. "
                "You specialize in how AI-generated outputs affect public trust, knowledge quality, and online ecosystems. "
                "You evaluate whether the rationale correctly identifies risks related to hallucination, citation of non-existent sources, deceptive framing, or viral propagation of falsehoods. "
                "You draw from disciplines like computational journalism, content moderation, and political communication to assess how harmful or misleading content can spread through or originate from generative systems."
            ),
            "Copyright, IP & Originality": (
                "You are a digital copyright and intellectual property law expert. "
                "You specialize in the legality and ethics of training, generating, and redistributing content using AI models. "
                "You assess whether the incident and rationales align with fair use principles, licensing agreements, and originality standards. "
                "You are particularly attuned to the unauthorized scraping of copyrighted data, AI-generated plagiarism, or commercial misuse of protected material. "
                "You ensure the rationale reflects the seriousness of IP infringement and evaluates the potential exposure of the new system to legal or reputational risk."
            ),
            "Ethics, Values & Appropriateness": (
                "You are a philosopher of technology and AI ethicist focused on the alignment of AI systems with societal values, moral norms, and human dignity. "
                "You consider whether the rationale reflects a deeper understanding of cultural sensitivities, anthropomorphic deception, inappropriate incentives, or scope creep. "
                "You assess whether incidents involve moral trade-offs, emotional manipulation, or conflict with ethical expectations of users. "
                "You bring experience from both deontological and consequentialist traditions, and help identify when 'technically feasible' may not mean 'socially acceptable.'"
            ),
            "Freedom of Expression & Human Rights": (
                "You are a human rights and civil liberties advocate specializing in freedom of speech, freedom of assembly, and access to information in digital systems. "
                "You assess whether rationales correctly interpret restrictions, censorship risks, and how expression rights might be enabled or undermined by AI. "
                "You evaluate the balance between preventing harm (e.g. hate speech, misinformation) and protecting individual rights. "
                "You are experienced in the international legal frameworks for rights-based assessments, and are sensitive to geopolitical or cultural variations in what constitutes legitimate expression."
            ),
            "Governance, Compliance & Legal": (
                "You are a legal and regulatory compliance specialist with expertise in AI policy, risk governance, liability, and industry best practices. "
                "You review whether the rationale reflects proper interpretation of legal constraints, ethical review gaps, or lack of due diligence. "
                "You focus on institutional responsibility, regulatory frameworks, licensing, risk disclosure, and liability mapping. "
                "You are also alert to systemic governance failuresâ€”whether through lack of oversight, policy evasion, or unclear boundaries of accountability in complex AI systems."
            )
        }
    
    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(3))
    def _call_openai(self, messages: List[Dict], temperature: float = 0.2, max_tokens: int = 600) -> str:
        """Call OpenAI API with retry logic"""
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
            )
            
            # Debug: Print response type and structure
            print(f"Response type: {type(response)}")
            
            # Handle different response formats
            if isinstance(response, str):
                # Gateway returned a string directly
                print("Gateway returned string response")
                print(f"String content (first 200 chars): {response[:200]}...")
                return response
            elif hasattr(response, 'choices') and response.choices:
                # Standard OpenAI response format
                content = response.choices[0].message.content
                print(f"Standard OpenAI response content (first 200 chars): {content[:200]}...")
                return content
            else:
                # Unknown format, try to handle gracefully
                print(f"Unknown response format: {response}")
                # If response is a dict, try to extract content
                if isinstance(response, dict):
                    # Try common fields
                    content = response.get('content') or response.get('message') or response.get('text')
                    if content:
                        print(f"Extracted content from dict: {content[:200]}...")
                        return content
                # If all else fails, convert to string
                return str(response)
                
        except Exception as e:
            print(f"Error calling OpenAI: {e}")
            print(f"Response object: {response if 'response' in locals() else 'No response'}")
            raise e
    
    def router_agent(self, is1: str, id1: str, pd1: str) -> Dict:
        """Router agent to classify risk type"""
        prompt = self.router_prompt + f"\nIS1: {is1}\nID1: {id1}\nPD1: {pd1}\n"
        messages = [
            {"role": "system", "content": "You are the Router Agent."},
            {"role": "user", "content": prompt}
        ]
        response = self._call_openai(messages)
        print(f"Router agent raw response: {response}")
        try:
            output = json_repair.loads(response)
            print(f"Router agent parsed output: {output}")
        except Exception as e:
            print(f"Error parsing router output: {e}")
            print(f"Raw response that failed to parse: {response}")
            output = {"risk_type": "Safety & Security", "justification": "Default due to parsing error"}
        return output
    
    def scorer_agent(self, is1: str, id1: str, pd1: str) -> Dict:
        """PhD student scorer agent"""
        # Add some context variation to make each call more unique
        variation_context = f"""
        
IMPORTANT ANALYSIS NOTES:
- Pay careful attention to the specific technologies mentioned
- Consider the exact use case and application domain
- Look for subtle but important differences between systems
- Don't give similar scores to different incidents - each should be evaluated independently
- Be critical about transferability - many incidents will have low transferability
        
Original Incident Context: {is1}
Specific Incident Details: {id1}
Target Product Details: {pd1}
        """
        
        prompt = self.phd_student_prompt.replace("{IS1}", is1).replace("{ID1}", id1).replace("{PD1}", pd1) + variation_context
        messages = [
            {"role": "system", "content": "You are the Scorer Agent (PhD Student). Be thorough and discriminating in your analysis. Each incident should get unique scores based on its specific characteristics."},
            {"role": "user", "content": prompt}
        ]
        response = self._call_openai(messages, temperature=0.4, max_tokens=800)  # Increased temperature and tokens for more variation
        print(f"Scorer agent raw response: {response}")
        try:
            output = json_repair.loads(response)
            print(f"Scorer agent parsed output: {output}")
        except Exception as e:
            print(f"Error parsing scorer output: {e}")
            print(f"Raw response that failed to parse: {response}")
            output = self._get_default_scores()
        return output
    
    def _get_default_scores(self) -> Dict:
        """Default scores when parsing fails"""
        return {
            "Logical Coherence": [3, "Default score due to parsing error"],
            "Factual Accuracy": [3, "Default score due to parsing error"],
            "Practical Implementability": [3, "Default score due to parsing error"],
            "Contextual Relevance": [3, "Default score due to parsing error"],
            "Impact": [3, "Default score due to parsing error"],
            "Exploitability": [3, "Default score due to parsing error"],
            "Application Domain": "General AI"
        }
    
    def calculate_authentic_prism_scores(
        self, 
        incident_data: Dict[str, Any], 
        product_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Calculate PRISM scores using 6-dimension methodology
        """
        
        # Extract data
        is1 = incident_data.get('system_name', 'Unknown System')
        id1 = incident_data.get('description', '')
        pd1 = product_data.get('description', '')
        
        print("Step 1: Running Router Agent...")
        router_result = self.router_agent(is1, id1, pd1)
        risk_type = router_result.get("risk_type", "Safety & Security")
        print(f"Risk Type: {risk_type}")

        print("Step 2: Running Scorer Agent...")
        scorer_result = self.scorer_agent(is1, id1, pd1)
        
        # Extract scores and rationales from the 6-dimension result
        scores = {}
        rationales = {}
        
        dimension_mapping = {
            'Logical Coherence': 'logical_coherence',
            'Factual Accuracy': 'factual_accuracy',
            'Practical Implementability': 'practical_implementability',
            'Contextual Relevance': 'contextual_relevance',
            'Impact': 'impact',
            'Exploitability': 'exploitability'
        }
        
        for dimension, mapped_name in dimension_mapping.items():
            score_rationale = scorer_result.get(dimension, [3, "Default score"])
            if isinstance(score_rationale, list) and len(score_rationale) >= 2:
                scores[mapped_name] = score_rationale[0]
                rationales[mapped_name] = score_rationale[1]
            else:
                scores[mapped_name] = 3
                rationales[mapped_name] = "Default score"
        
        # Calculate transferability score using authentic methodology
        transferability_score = self._calculate_transferability_score(scores)
        
        # Create comprehensive result
        result = {
            'prism_scores': scores,
            'prism_rationales': rationales,
            'transferability_score': transferability_score,
            'application_domain': scorer_result.get('Application Domain', 'General AI'),
            'risk_type': risk_type,
            'risk_justification': router_result.get('justification', ''),
            'confidence_score': 0.9,  # High confidence for LLM-based scoring
            'scoring_method': 'authentic_6dim'
        }
        
        return result
    
    def _calculate_transferability_score(self, scores: Dict[str, int]) -> float:
        """Calculate transferability score using research methodology"""
        
        # Weighted scoring based on research adapted for 6 dimensions
        weights = {
            'logical_coherence': 0.25,
            'factual_accuracy': 0.25,
            'practical_implementability': 0.20,
            'contextual_relevance': 0.15,
            'impact': 0.10,
            'exploitability': 0.05
        }
        
        total_score = 0
        total_weight = 0
        
        for dimension, score in scores.items():
            if dimension in weights:
                weight = weights[dimension]
                # Normalize score to 0-1 range
                normalized_score = (score - 1) / 4  # Convert 1-5 to 0-1
                total_score += normalized_score * weight
                total_weight += weight
        
        if total_weight > 0:
            # Return as 1-5 scale score
            return min(5, max(1, (total_score / total_weight) * 4 + 1))
        else:
            return 3.0  # Neutral score
    
    def get_explanation(self, result: Dict[str, Any], incident: Dict[str, Any], product: Dict[str, Any]) -> str:
        """Generate explanation using authentic 6-dimension results"""
        
        transferability = result['transferability_score']
        scores = result['prism_scores']
        rationales = result['prism_rationales']
        
        explanation = f"ðŸŽ¯ PRISM Transferability Score: {transferability:.1f}/5\\n\\n"
        explanation += f"ðŸ·ï¸ Application Domain: {result['application_domain']}\\n"
        explanation += f"âš ï¸ Risk Type: {result['risk_type']}\\n\\n"
        
        # 6-Dimension breakdown
        explanation += "ðŸ“Š **6-Dimension PRISM Analysis:**\\n\\n"
        
        # Core Dimensions
        logical_score = scores.get('logical_coherence', 3)
        explanation += f"ðŸ”— **Logical Coherence**: {logical_score}/5\\n"
        explanation += f"   {rationales.get('logical_coherence', 'No rationale available')}\\n\\n"
        
        factual_score = scores.get('factual_accuracy', 3)
        explanation += f"âœ… **Factual Accuracy**: {factual_score}/5\\n"
        explanation += f"   {rationales.get('factual_accuracy', 'No rationale available')}\\n\\n"
        
        implement_score = scores.get('practical_implementability', 3)
        explanation += f"ðŸ› ï¸ **Practical Implementability**: {implement_score}/5\\n"
        explanation += f"   {rationales.get('practical_implementability', 'No rationale available')}\\n\\n"
        
        context_score = scores.get('contextual_relevance', 3)
        explanation += f"ðŸŽ¯ **Contextual Relevance**: {context_score}/5\\n"
        explanation += f"   {rationales.get('contextual_relevance', 'No rationale available')}\\n\\n"
        
        # Impact (Single Dimension)
        impact_score = scores.get('impact', 3)
        explanation += f"ðŸ’¥ **Impact**: {impact_score}/5\\n"
        explanation += f"   {rationales.get('impact', 'No rationale available')}\\n\\n"
        
        # Exploitability
        exploit_score = scores.get('exploitability', 3)
        explanation += f"ðŸŽ­ **Exploitability**: {exploit_score}/5\\n"
        explanation += f"   {rationales.get('exploitability', 'No rationale available')}\\n\\n"
        
        # Risk Assessment
        if transferability >= 4:
            explanation += "ðŸ”¥ **HIGH RISK**: Strong potential for incident transfer\\n"
        elif transferability >= 3:
            explanation += "âš ï¸ **MODERATE RISK**: Some potential for incident transfer\\n"
        else:
            explanation += "â„¹ï¸ **LOW RISK**: Limited potential for incident transfer\\n"
        
        explanation += f"\\nðŸ“ **Risk Justification**: {result['risk_justification']}"
        
        return explanation

    def calculate_generic_confidence_score(
        self, 
        incident_data: Dict[str, Any], 
        product_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Calculate generic confidence score using LLM for non-PRISM mode
        """
        try:
            # Extract data
            product_name = product_data.get('name', 'Unknown Product')
            product_description = product_data.get('description', '')
            incident_description = incident_data.get('description', '')
            context = incident_data.get('context', '')
            
            # Create generic scoring prompt
            prompt = f"""
You are an AI safety expert analyzing incident transferability between AI systems.

PRODUCT TO ANALYZE:
Name: {product_name}
Description: {product_description}

INCIDENT TO EVALUATE:
Description: {incident_description}

ANALYSIS CONTEXT: {context}

TASK: Rate how likely this incident is to occur with the given product on a scale of 1-5.

SCORING GUIDELINES:
- 5 = VERY HIGH likelihood (same/similar technology, same use case, incident very likely to happen)
- 4 = HIGH likelihood (related technology, similar context, incident quite possible)
- 3 = MODERATE likelihood (some shared elements, incident somewhat possible)
- 2 = LOW likelihood (few shared elements, incident unlikely but theoretically possible)
- 1 = VERY LOW likelihood (completely different technology/context, incident very unlikely)

CRITICAL ANALYSIS FACTORS:
1. Technology overlap: Do they use similar AI technologies?
2. Use case similarity: Are the application domains related?
3. User interaction patterns: Similar ways users interact with the systems?
4. Risk surface area: Do they have similar potential failure modes?
5. Context relevance: Would this incident make sense in the product's context?

BE VERY DISCRIMINATING in your scoring. Consider these specific aspects:
- If technologies are completely different (e.g., image generation vs text processing), score lower
- If use cases are different (e.g., creative tools vs business applications), score lower  
- If the incident involves specific features the product doesn't have, score lower
- If the incident is about a specific implementation detail, check if the product has that detail
- Consider the severity and specificity of the incident - generic issues transfer more than specific ones

IMPORTANT: Each incident should get a unique score based on its specific characteristics. Don't default to middle scores.

Provide your response in this JSON format:
{{
  "confidence_score": <number 1-5>,
  "reasoning": "<specific analysis mentioning technology overlap, use case similarity, and likelihood assessment>"
}}
"""

            messages = [
                {"role": "system", "content": "You are an expert at assessing AI incident transferability for generic analysis."},
                {"role": "user", "content": prompt}
            ]
            
            response = self._call_openai(messages, temperature=0.3, max_tokens=300)
            
            print(f"Generic confidence raw response: {response}")
            try:
                output = json_repair.loads(response)
                print(f"Generic confidence parsed output: {output}")
                confidence_score = output.get('confidence_score', 3)
                reasoning = output.get('reasoning', 'Generic confidence assessment')
                
                # Debug: Show the actual score being returned
                print(f"Final confidence score: {confidence_score} (raw: {confidence_score}, type: {type(confidence_score)})")
                
            except Exception as e:
                print(f"Error parsing generic scoring output: {e}")
                print(f"Raw response that failed to parse: {response}")
                confidence_score = 3
                reasoning = "Default score due to parsing error"
            
            # Create result in same format as PRISM for consistency
            result = {
                'transferability_score': confidence_score,
                'confidence_score': confidence_score,
                'reasoning': reasoning,
                'scoring_method': 'generic_llm'
            }
            
            return result
            
        except Exception as e:
            print(f"Error in calculate_generic_confidence_score: {e}")
            return {
                'transferability_score': 3.0,
                'confidence_score': 3.0,
                'reasoning': 'Error in calculation',
                'scoring_method': 'generic_llm'
            }

    def test_gateway(self):
        """Test the OpenAI gateway configuration"""
        try:
            print(f"Testing gateway connection...")
            print(f"API Key: {self.client.api_key[:15] if self.client.api_key else 'None'}...")
            print(f"Base URL: {self.client.base_url}")
            
            # Try a simple completion request
            response = self.client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": "Hello, this is a test. Please respond with just 'OK'."}],
                max_tokens=10
            )
            
            print(f"Test response type: {type(response)}")
            print(f"Test response content: {response}")
            
            if hasattr(response, 'choices') and response.choices:
                print(f"SUCCESS: Gateway is working correctly")
                print(f"Response: {response.choices[0].message.content}")
                return True
            else:
                print(f"ERROR: Gateway returned unexpected format")
                return False
                
        except Exception as e:
            print(f"ERROR: Gateway test failed: {e}")
            return False

    def bulk_calculate_generic_scores(self, incidents: list, product_data: dict, context: str) -> list:
        """
        Calculate generic confidence scores for ALL incidents in ONE API call.
        Returns scores on 1-100 scale.
        """
        try:
            print(f"Bulk processing {len(incidents)} incidents for generic scoring")
            
            # Create bulk prompt with all incidents
            incidents_text = ""
            for i, incident in enumerate(incidents, 1):
                incidents_text += f"""
INCIDENT {i}:
ID: {incident['id']}
Title: {incident['title']}
Description: {incident['description']}
Technologies: {', '.join(incident.get('technologies', []))}

"""

            prompt = f"""
You are an AI safety expert analyzing incident transferability between AI systems.

PRODUCT TO ANALYZE:
Name: {product_data.get('name', 'Unknown Product')}
Description: {product_data.get('description', '')}

CONTEXT: {context}

TASK: For each incident below, rate how likely it is to occur with the given product on a scale of 1-100.

SCORING GUIDELINES (1-100 scale):
- 90-100 = VERY HIGH likelihood (same/similar technology, same use case, incident very likely to happen)
- 70-89 = HIGH likelihood (related technology, similar context, incident quite possible)
- 50-69 = MODERATE likelihood (some shared elements, incident somewhat possible)
- 30-49 = LOW likelihood (few shared elements, incident unlikely but theoretically possible)
- 1-29 = VERY LOW likelihood (completely different technology/context, incident very unlikely)

BE VERY DISCRIMINATING in your scoring. Consider these specific aspects:
- If technologies are completely different (e.g., image generation vs text processing), score lower
- If use cases are different (e.g., creative tools vs business applications), score lower  
- If the incident involves specific features the product doesn't have, score lower
- Consider the severity and specificity of the incident

INCIDENTS TO ANALYZE:
{incidents_text}

Provide your response in this JSON format:
{{
  "incident_scores": [
    {{
      "incident_id": <number>,
      "confidence_score": <number 1-100>,
      "reasoning": "<specific analysis for this incident>"
    }},
    ...for each incident...
  ]
}}
"""

            messages = [
                {"role": "system", "content": "You are an expert at assessing AI incident transferability. Provide varied, discriminating scores from 1-100."},
                {"role": "user", "content": prompt}
            ]
            
            response = self._call_openai(messages, temperature=0.4, max_tokens=2000)
            print(f"Bulk generic raw response: {response[:500]}...")
            
            try:
                output = json_repair.loads(response)
                scores = output.get('incident_scores', [])
                
                # Validate and format results
                results = []
                for i, incident in enumerate(incidents):
                    if i < len(scores):
                        score_data = scores[i]
                        results.append({
                            'incident_id': incident['id'],
                            'confidence_score': score_data.get('confidence_score', 50),
                            'reasoning': score_data.get('reasoning', 'Generic analysis')
                        })
                    else:
                        # Fallback if not enough scores returned
                        results.append({
                            'incident_id': incident['id'], 
                            'confidence_score': 50,
                            'reasoning': 'Default score - insufficient LLM response'
                        })
                
                print(f"Bulk generic results: {len(results)} scores, range {min(r['confidence_score'] for r in results)}-{max(r['confidence_score'] for r in results)}")
                return results
                
            except Exception as e:
                print(f"Error parsing bulk generic response: {e}")
                # Return default varied scores
                return [{'incident_id': inc['id'], 'confidence_score': 50 + (i * 5) % 40, 'reasoning': 'Parsing error'} 
                       for i, inc in enumerate(incidents)]
                
        except Exception as e:
            print(f"Error in bulk_calculate_generic_scores: {e}")
            return [{'incident_id': inc['id'], 'confidence_score': 50, 'reasoning': 'Calculation error'} 
                   for inc in incidents]

    def bulk_calculate_prism_scores(self, incidents: list, product_data: dict, context: str) -> list:
        """
        Calculate PRISM scores for ALL incidents in ONE API call.
        Returns scores on 1-100 scale with proper weighting: [0.2, 0.2, 0.2, 0.2, 0.1, 0.1]
        """
        try:
            print(f"Bulk processing {len(incidents)} incidents for PRISM scoring")
            
            # Create bulk prompt with all incidents
            incidents_text = ""
            for i, incident in enumerate(incidents, 1):
                incidents_text += f"""
INCIDENT {i}:
ID: {incident['id']}
Title: {incident['title']}
Description: {incident['description']}
Technologies: {', '.join(incident.get('technologies', []))}

"""

            prompt = f"""
You are an expert PhD student working in AI Ethics and risks, using the PRISM methodology.

PRODUCT TO ANALYZE:
Name: {product_data.get('name', 'Unknown Product')}
Description: {product_data.get('description', '')}

CONTEXT: {context}

TASK: For each incident below, provide PRISM scores on 1-100 scale for all 6 dimensions:

SCORING GUIDELINES (1-100 scale for each dimension):
1. Logical Coherence: Does the incident logically fit the product's function?
2. Factual Accuracy: Is the incident within scope of product's features/technology?
3. Practical Implementability: How likely is the incident to occur in real-world?
4. Contextual Relevance: Does this make sense in the product's application domain?
5. Impact: How severe is the overall impact (individual/group/global)?
6. Exploitability: Is the risk inherent to system or from user misuse?

BE DISCRIMINATING - each incident should get unique scores based on specific characteristics.

INCIDENTS TO ANALYZE:
{incidents_text}

Provide your response in this JSON format:
{{
  "incident_scores": [
    {{
      "incident_id": <number>,
      "logical_coherence": <number 1-100>,
      "factual_accuracy": <number 1-100>,
      "practical_implementability": <number 1-100>,
      "contextual_relevance": <number 1-100>,
      "impact": <number 1-100>,
      "exploitability": <number 1-100>,
      "reasoning": "<brief analysis for this incident>"
    }},
    ...for each incident...
  ]
}}
"""

            messages = [
                {"role": "system", "content": "You are a PRISM methodology expert. Provide varied, discriminating scores from 1-100 for each dimension."},
                {"role": "user", "content": prompt}
            ]
            
            response = self._call_openai(messages, temperature=0.5, max_tokens=3000)
            print(f"Bulk PRISM raw response: {response[:500]}...")
            
            try:
                output = json_repair.loads(response)
                scores = output.get('incident_scores', [])
                
                # Calculate weighted overall scores with new weights: [0.2, 0.2, 0.2, 0.2, 0.1, 0.1]
                weights = {
                    'logical_coherence': 0.2,
                    'factual_accuracy': 0.2,
                    'practical_implementability': 0.2,
                    'contextual_relevance': 0.2,
                    'impact': 0.1,
                    'exploitability': 0.1
                }
                
                # Validate and format results
                results = []
                for i, incident in enumerate(incidents):
                    if i < len(scores):
                        score_data = scores[i]
                        
                        # Get individual dimension scores
                        dim_scores = {
                            'logical_coherence': score_data.get('logical_coherence', 50),
                            'factual_accuracy': score_data.get('factual_accuracy', 50),
                            'practical_implementability': score_data.get('practical_implementability', 50),
                            'contextual_relevance': score_data.get('contextual_relevance', 50),
                            'impact': score_data.get('impact', 50),
                            'exploitability': score_data.get('exploitability', 50)
                        }
                        
                        # Calculate weighted overall score
                        overall_score = sum(dim_scores[dim] * weights[dim] for dim in dim_scores.keys())
                        
                        results.append({
                            'incident_id': incident['id'],
                            'logical_coherence': dim_scores['logical_coherence'],
                            'factual_accuracy': dim_scores['factual_accuracy'],
                            'practical_implementability': dim_scores['practical_implementability'],
                            'contextual_relevance': dim_scores['contextual_relevance'],
                            'impact': dim_scores['impact'],
                            'exploitability': dim_scores['exploitability'],
                            'overall_score': overall_score,
                            'reasoning': score_data.get('reasoning', 'PRISM analysis')
                        })
                    else:
                        # Fallback if not enough scores returned
                        results.append({
                            'incident_id': incident['id'],
                            'logical_coherence': 50,
                            'factual_accuracy': 50,
                            'practical_implementability': 50,
                            'contextual_relevance': 50,
                            'impact': 50,
                            'exploitability': 50,
                            'overall_score': 50,
                            'reasoning': 'Default score - insufficient LLM response'
                        })
                
                print(f"Bulk PRISM results: {len(results)} scores, overall range {min(r['overall_score'] for r in results):.1f}-{max(r['overall_score'] for r in results):.1f}")
                return results
                
            except Exception as e:
                print(f"Error parsing bulk PRISM response: {e}")
                # Return default varied scores
                return [{'incident_id': inc['id'], 'logical_coherence': 50, 'factual_accuracy': 50, 
                        'practical_implementability': 50, 'contextual_relevance': 50, 'impact': 50, 
                        'exploitability': 50, 'overall_score': 50, 'reasoning': 'Parsing error'} 
                       for inc in incidents]
                
        except Exception as e:
            print(f"Error in bulk_calculate_prism_scores: {e}")
            return [{'incident_id': inc['id'], 'logical_coherence': 50, 'factual_accuracy': 50, 
                    'practical_implementability': 50, 'contextual_relevance': 50, 'impact': 50, 
                    'exploitability': 50, 'overall_score': 50, 'reasoning': 'Calculation error'} 
                   for inc in incidents]

# Legacy compatibility functions
def calculate_prism_scores(incident_data: Dict, product_data: Dict) -> Dict:
    """Legacy function for backward compatibility"""
    scorer = PRISMScorer()
    return scorer.calculate_authentic_prism_scores(incident_data, product_data)

def get_explanation(result: Dict, incident: Dict, product: Dict) -> str:
    """Legacy function for backward compatibility"""
    scorer = PRISMScorer()
    return scorer.get_explanation(result, incident, product) 